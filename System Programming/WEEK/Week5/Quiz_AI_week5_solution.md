# 2025 Spring SNU System Programming - Weekly Quiz (Week 5, with Solutions)

We have prepared some light quizzes related to class content of week 5.  
These problems have been selected from those generated by **AI tool (Cramify)**,  
based on your class ppt content.

There is no need to submit anything after reviewing this material,  
but we hope it will help you review the class content you learned this week and prepare for the quiz.

---

Weekly Quiz has two documents, one with only the problems and  
the other with both the problems and solutions.  

# Question 1

Modern operating systems utilize virtual memory to address several key challenges related to memory management.  
Which of the following problems does virtual memory not directly solve?

A) Limited storage capacity on persistent storage devices.  
B) Managing the allocation of physical memory to multiple processes.  
C) Protecting processes from accessing each other's memory.  
D) Facilitating shared memory regions between processes.  

**Answer: A**

**Solution:**  
Virtual memory primarily solves problems related to limited physical memory, memory management across multiple processes, protection from unauthorized access, and memory sharing between processes.  
It does not directly manage data persistence or storage capacity, which are handled by file systems and storage devices.  
It creates an abstraction layer over physical memory, allowing each process to operate within its own isolated address space.  
This abstraction enables efficient memory allocation, protection, and sharing without the constraints of physical memory limitations.

# Question 2

A process attempts to write to a memory page marked as read-only.  
Describe the most likely immediate system response and explain how this mechanism enhances system stability and security.

A) The write operation proceeds, but the data is silently discarded, preventing data corruption but potentially leading to application errors.  
B) The process is immediately terminated without any error message, preventing further damage but potentially losing unsaved data.  
C) The system prompts the user for permission to allow the write operation, enhancing security by requiring explicit authorization.  
D) A segmentation fault occurs, preventing memory corruption and protecting system stability and security by isolating processes.  

**Answer: D**

**Solution:**  
The system will trigger a segmentation fault (or similar memory access violation exception).  
This prevents the process from corrupting memory that it shouldn't modify, which could lead to unpredictable behavior or system crashes.  
By enforcing access permissions, the system isolates processes, preventing one faulty or malicious process from affecting others or the operating system itself.  
This contributes to both stability and security.

# Question 3

A program frequently accesses memory locations 0x1000, 0x1004, 0x1008, and 0x100C.  
Later, it accesses 0x2000, 0x2004, 0x2008, and 0x200C.  
Which of the following best describes the program's memory access pattern and its impact on cache effectiveness if the cache block size is 16 bytes?

A) The program demonstrates spatial locality, and a 16-byte cache block size effectively utilizes this pattern.  
B) The program's access pattern exhibits no locality, and cache block size is irrelevant.  
C) The program exhibits spatial locality, but a smaller cache block size would be more efficient.  
D) The program demonstrates temporal locality, and a larger cache block size would further improve performance.  

**Answer: A**

**Solution:**  
The program exhibits both temporal and spatial locality.  
The accesses to 0x1000, 0x1004, 0x1008, and 0x100C are close together (spatial locality).  
Since the cache block size is 16 bytes, accessing 0x1000 will likely bring in 0x1004, 0x1008, and 0x100C into the same cache block.  
The later accesses to 0x2000, 0x2004, 0x2008, and 0x200C also exhibit spatial locality.  
Both sets of accesses exploit spatial locality, leading to fewer cache misses and improved performance.  
While there might be temporal locality within each set (e.g., 0x1000 accessed multiple times), the question focuses on the relationship between the access pattern and the cache block size.

# Question 4

Regarding cache replacement policies, which of the following statements are true?  
Indicate True or False for each statement.

a) The Least Recently Used (LRU) policy always achieves the lowest possible miss rate for a given cache size.  

**Answer: False**  

**Solution:**  
While LRU performs well due to its exploitation of temporal locality, it is not guaranteed to be optimal.  
An optimal policy would require knowledge of future accesses, which is impossible in practice.  
LRU can perform poorly in cases of cyclical access patterns that exceed the cache size.

b) Approximations of LRU, such as clock algorithms, aim to reduce the complexity of tracking the exact order of accesses while maintaining reasonable performance.  

**Answer: True**  

**Solution:**  
LRU approximations, like the clock algorithm, simplify tracking by using bits to indicate recent usage.  
This reduces overhead compared to maintaining a full access history, making them more practical for hardware implementation.

c) A capacity miss occurs when the cache is too small to hold the working set of data, regardless of the replacement policy used.  

**Answer: True**  

**Solution:**  
A capacity miss is inherently due to the limited size of the cache and not the replacement policy.  
When the working set exceeds the cache size, misses will occur regardless of how effectively the replacement policy manages the limited space.

d) In a fully associative cache, all replacement policies perform identically because any block can be placed anywhere in the cache.  

**Answer: True**  

**Solution:**  
In a fully associative cache, any block can reside in any cache location.  
This eliminates the issue of conflict misses, which are affected by placement policies.  
Therefore, different replacement policies become equivalent in terms of miss rate.

e) The choice of cache replacement policy has no impact on the number of cold misses.  

**Answer: True**  

**Solution:**  
Cold misses occur on the first access to a block, regardless of the replacement policy.  
Since the block is not yet in the cache, the policy has no bearing on whether a miss occurs.

# Question 5

Consider a system with a fixed-size cache.  
If the program exhibits high spatial locality, increasing the cache block size is likely to reduce which type of cache miss, and why?  
Conversely, if the program exhibits low spatial locality, increasing the cache block size may increase which type of cache miss, and why?

A) High spatial locality: increases hit rate by improving data availability; Low spatial locality: decreases hit rate due to increased cache pollution from unused data.  
B) High spatial locality: reduces capacity misses by increasing effective cache size; Low spatial locality: increases compulsory misses due to fetching unnecessary data.  
C) High spatial locality: reduces conflict misses by distributing data across more sets; Low spatial locality: increases conflict misses due to more blocks mapping to the same set.  
D) High spatial locality: reduces compulsory misses by prefetching adjacent data; Low spatial locality: increases capacity misses due to reduced effective cache size.  

**Answer: D**  

**Solution:**  
With high spatial locality, increasing the block size is likely to reduce **compulsory(cold) misses**.  
This is because fetching a larger block brings in adjacent data that is likely to be accessed soon, exploiting the spatial locality.  
With low spatial locality, increasing the block size may increase **capacity misses**.  
Larger blocks mean fewer blocks fit in the cache.  
If the program isn't using the extra data brought in by the larger block, this reduces the effective cache size and leads to more evictions and capacity misses.

# Question 6

You are developing a program that processes large datasets where the exact size is unknown until runtime.  
Explain why using dynamic memory allocation with functions like `malloc()` is more advantageous than relying on statically allocated global variables in this scenario.  
Specifically, address the limitations of static allocation and how dynamic allocation overcomes them.

**Answer:**  
Dynamic memory allocation allows for flexible memory management at runtime, accommodating data sizes unknown at compile time.  
Static allocation, with fixed sizes, risks buffer overflows or memory waste when data sizes vary during program execution.

**Solution:**  
Static allocation requires the size of data structures to be known at compile time.  
This is a significant limitation when dealing with datasets whose size varies during program execution.  
If you declare a global array of a fixed size, it might be too small to hold the incoming data, leading to buffer overflows.  
Conversely, it could be excessively large, wasting memory if the actual data is much smaller.  
Dynamic memory allocation, using functions like `malloc()`, allows you to allocate memory as needed during runtime.  
This flexibility ensures that you allocate only the necessary amount of memory, preventing both buffer overflows and memory waste.  
You can adjust the allocated memory size as the program runs, accommodating varying data sizes efficiently.

# Question 7

In C, when dynamically allocating memory using functions like `malloc()` and `realloc()`, where is this memory allocated, and how is this memory region managed?  
Explain the potential issues that can arise if this region is mismanaged.

**Answer:**  
Dynamically allocated memory is allocated on the heap, managed by the dynamic memory allocator in the C standard library using system calls like `sbrk()` or `brk()`.  
Mismanagement can lead to memory leaks, dangling pointers, and heap corruption.

**Solution:**  
Dynamically allocated memory in C is allocated on the heap.  
This region is managed by the dynamic memory allocator, a part of the C standard library (`libc`).  
The libc uses system calls like `sbrk()` or `brk()` to request memory from the operating system's kernel, expanding the heap as needed.  
Mismanagement of the heap can lead to memory leaks (failing to `free()` allocated memory), dangling pointers (accessing freed memory), and heap corruption (writing beyond allocated boundaries), causing program crashes or unpredictable behavior.

# Question 8

A dynamic memory allocator allocates memory blocks in multiples of 8 bytes.  
A program requests 26 bytes of memory, and the allocator provides a 32-byte block. Later, the program frees this block.  
Another program then requests 12 bytes, and although 16 bytes (32 - 26 + 8 = 16 bytes) are technically available across the freed 32-byte block and other smaller free blocks scattered throughout the heap, no single free block is large enough to fulfill the 12-byte request.  
Identify the types of fragmentation present in this scenario.

A) Both internal and external fragmentation are present.  
B) Only internal fragmentation is present.  
C) Neither internal nor external fragmentation is present.  
D) Only external fragmentation is present.  

**Answer: A**  

**Solution:**  
1. Internal Fragmentation:  
The initial request for 26 bytes resulted in a 32-byte block being allocated.  
The difference (32 - 26 = 6 bytes) represents internal fragmentation, where allocated memory within a block remains unused.
2. External Fragmentation:  
Despite having a total of 16 free bytes after the 32-byte block is freed, the allocator cannot fulfill the 12-byte request because these bytes are scattered across multiple non-contiguous free blocks.  
This is external fragmentation, where enough free memory exists in total, but not in a single contiguous block.

# Question 9

| Operation | Argument (bytes) | Return Value (Address)	|
|-------|-------|-------|
| malloc | 32 | 0x1000 |
| malloc | 16	| 0x1020 |
| malloc | 48 | 0x1030 |
| free | 0x1030 | - |
| free | 0x1020 | - |
| malloc | 24 | ? |

Consider a K&R-style dynamic memory allocator with a base size of 16 bytes.  
The table above shows a sequence of calls to malloc and free.  
What address will the fourth malloc call (requesting 24 bytes) return?  
Assume that the heap starts at address 0x1000 and that the allocator uses a first-fit strategy and coalesces adjacent free blocks.

**Answer:**  
0x1020

**Solution:**  
1. Initial Allocations:  
The first three `malloc` calls allocate blocks of 32, 16, and 48 bytes at addresses 0x1000, 0x1020, and 0x1030, respectively.  
Note that the requested sizes are rounded up to the nearest multiple of the base size (16 bytes).
2. free(0x1030), free(0x1020):  
This frees 48, 16 bytes block at address 0x1030, 0x1020, respectively, creating free blocks.
These free blocks are adjacent, so coalesces in on 64 bytes free block.
3. malloc(24):  
The fourth `malloc` call requests 24 bytes, which is rounded up to 32 bytes.  
The allocator searches for the first free block that can accommodate this request. The free block at 0x1020 is large enough. (64 bytes)
The allocator then allocates the first 32 bytes of this combined free block. Thus, the return address is 0x1020.

# Question 10

In dynamic memory allocation, explain the benefits of employing a circular linked list to manage free blocks, particularly highlighting its impact on handling the 'end' of the list and potential performance optimizations related to the list's head.

**Answer:**  
Circular linked lists simplify list management by removing the need for special "end" handling.  
Dynamically updating the head to the last found block optimizes first-fit allocation by potentially finding larger blocks faster further down the list.

**Solution:**  
A circular linked list eliminates the need for special handling of the list's end because each node points to the next, eventually looping back to the beginning.  
This simplifies the code for traversing and manipulating the list.  
For performance, the head of the list can be dynamically updated to the location where the last block was found.  
This optimization increases the likelihood of quickly finding suitable free blocks in subsequent allocations, as larger blocks might be located further down the list.  
This 'moving head' strategy improves the efficiency of the first-fit allocation approach.

# Question 11

A program requests 48 bytes of memory using `malloc()`.  
The memory allocator uses a base size of 16 bytes for allocation and maintains a linked list of free blocks.  
The allocator finds a free block of 112 bytes.  
Describe the steps the allocator takes to fulfill the request and manage the remaining free space.

**Answer:**  
The 112-byte block is split into a 48-byte allocated block and a 64-byte free block, which is added back to the free list.

**Solution:**  
1. Calculate the actual allocation size:  
The requested size (48 bytes) is rounded up to the nearest multiple of the base size (16 bytes).  
In this case, 48 bytes is already a multiple of 16, so the allocation size remains 48 bytes.
2. Split the free block:  
The allocator splits the 112-byte free block into two parts:
 - Allocated block: A 48-byte block is allocated to fulfill the request.
 - Remaining free block: The remaining portion of the original free block (112 - 48 = 64 bytes) becomes a new free block.
3. Update the free list:  
The original 112-byte free block is removed from the linked list.  
The new 64-byte free block is inserted into the linked list, maintaining the order of increasing memory addresses.

