{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (25.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: scipy==1.10.1 in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scipy==1.10.1) (1.26.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\c\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install opencv-python scikit-learn matplotlib scipy==1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense SIFT features\n",
      "Computing BoW histograms for training images\n",
      "Computing BoW histograms for test images\n",
      "Running KNN classifier\n",
      "Extracting dense SIFT features\n",
      "Computing BoW histograms for training images\n",
      "Computing BoW histograms for test images\n",
      "Running SVM classifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import stats\n",
    "from pathlib import Path, PureWindowsPath\n",
    "import random\n",
    "\n",
    "\n",
    "def extract_dataset_info(data_path):\n",
    "    # To do\n",
    "    data_path = Path(data_path)\n",
    "    # Get the absolute path of the data directory\n",
    "    train_path = data_path / 'train'\n",
    "    test_path = data_path / 'test'\n",
    "\n",
    "    # sort the classes in the alphabetical order\n",
    "    label_classes = sorted([d.name for d in train_path.iterdir() if d.is_dir()])\n",
    "    img_train_list, label_train_list = [], []\n",
    "    img_test_list, label_test_list = [], []\n",
    "\n",
    "    # Iterate through each class directory and collect image paths and labels\n",
    "    for label in label_classes:\n",
    "        label_train_dir = train_path / label\n",
    "        label_test_dir = test_path / label\n",
    "        \n",
    "        # append the training image paths and labels to the lists\n",
    "        for img_path in label_train_dir.iterdir():\n",
    "            img_train_list.append(str(img_path))\n",
    "            label_train_list.append(label_classes.index(label))\n",
    "\n",
    "        # append the testing image paths and labels to the lists\n",
    "        for img_path in label_test_dir.iterdir():\n",
    "            img_test_list.append(str(img_path))\n",
    "            label_test_list.append(label_classes.index(label))\n",
    "\n",
    "    return label_classes, label_train_list, img_train_list, label_test_list, img_test_list\n",
    "\n",
    "def compute_dsift(img):\n",
    "    # To do\n",
    "    # use the SIFT descriptor to extract features\n",
    "    sift = cv2.SIFT_create()\n",
    "    # step_size means the distance between two keypoints\n",
    "    step_size = 8\n",
    "    # patch_size means the size of the keypoint\n",
    "    patch_size = 16\n",
    "    # create a dense grid of keypoints\n",
    "    # the keypoints are evenly spaced in the image, with a size of patch_size\n",
    "    kp = [cv2.KeyPoint(x + patch_size / 2, y + patch_size / 2, patch_size)\n",
    "          for y in range(0, img.shape[0] - patch_size, step_size)\n",
    "          for x in range(0, img.shape[1] - patch_size, step_size)]\n",
    "    # compute(): calculate the SIFT descriptors for the keypoints\n",
    "    # kp: the keypoints detected in the image\n",
    "    # dense_feature: the SIFT descriptors for the keypoint, num(keypoint)=n\n",
    "    kp, dense_feature = sift.compute(img, kp)\n",
    "    \n",
    "    # returns the (n,128) array of SIFT descriptors\n",
    "    return dense_feature\n",
    "\n",
    "\n",
    "def predict_knn(feature_train, label_train, feature_test, k):\n",
    "    # To do\n",
    "    # use the KNN classifier to predict the labels of the test features\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    # fit() function: fit the model to the training data\n",
    "    knn.fit(feature_train)\n",
    "    # kneighbors() function: find the k nearest neighbors of the test features\n",
    "    # return the closest distance and the k indices of the neighbors\n",
    "    _, indices = knn.kneighbors(feature_test)\n",
    "    label_train = np.array(label_train)\n",
    "\n",
    "    label_test_pred = []\n",
    "    # for each test feature, find the most common label among its k nearest neighbors\n",
    "    for idx in (indices):\n",
    "        nearest_labels = label_train[idx]\n",
    "        label=np.bincount(nearest_labels).argmax()\n",
    "        label_test_pred.append(label)\n",
    "    label_test_pred = np.array(label_test_pred)\n",
    "    \n",
    "    return label_test_pred\n",
    "\n",
    "\n",
    "def build_visual_dictionary(dense_feature_list, dic_size):\n",
    "    # To do\n",
    "    # combine all the dense SIFT features into a single array\n",
    "    all_features = np.vstack(dense_feature_list)\n",
    "    # use the KMeans algorithm to cluster the SIFT feataures\n",
    "    # set the number of getting clusters to dic_size for n_init times\n",
    "    kmeans = KMeans(n_clusters=dic_size, random_state=0)\n",
    "    kmeans.fit(all_features)\n",
    "    # get the cluster centers from the KMeans model\n",
    "    vocab = kmeans.cluster_centers_\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def compute_bow(feature, vocab):\n",
    "    # To do\n",
    "    dic_size = vocab.shape[0]\n",
    "    # use the NearestNeighbors algorithm to find the nearest visual word for each SIFT feature\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(vocab)\n",
    "    # find the nearest visual word for each SIFT feature\n",
    "    _, indices = nn.kneighbors(feature)\n",
    "\n",
    "    # make the histogram of visual words\n",
    "    # bow_feature: the histogram of visual words\n",
    "    bow_feature = np.zeros(dic_size, dtype=np.float32)\n",
    "    for idx in indices.flatten():\n",
    "        bow_feature[idx] += 1.0\n",
    "\n",
    "    # normalize the histogram\n",
    "    # calculate the L2 norm of the histogram\n",
    "    # divide the bow_feature by the L2 norm to normalize it\n",
    "    norm = np.linalg.norm(bow_feature)\n",
    "    if norm > 0:\n",
    "        bow_feature /= norm\n",
    "        \n",
    "    return bow_feature\n",
    "\n",
    "\n",
    "def classify_knn_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list):\n",
    "    # To do    \n",
    "    print(\"Extracting dense SIFT features\")\n",
    "    # dense SIFT features from training images\n",
    "    # print for debugging\n",
    "    train_features = []\n",
    "    for img_path in img_train_list:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        feat = compute_dsift(img)\n",
    "        train_features.append(feat)\n",
    "        \n",
    "    for dic_size in [550]:\n",
    "        vocab = build_visual_dictionary(train_features, dic_size)\n",
    "\n",
    "        print(\"Computing BoW histograms for training images\")\n",
    "        bow_train = []\n",
    "        for feat in train_features:\n",
    "            bow_train.append(compute_bow(feat, vocab))\n",
    "\n",
    "        print(\"Computing BoW histograms for test images\")\n",
    "        test_features = []\n",
    "        for img_path in img_test_list:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            feat = compute_dsift(img)\n",
    "            test_features.append(feat)\n",
    "\n",
    "        bow_test = []\n",
    "        for feat in test_features:\n",
    "            bow_test.append(compute_bow(feat, vocab))\n",
    "\n",
    "        print(\"Running KNN classifier\")\n",
    "        pred = predict_knn(bow_train, label_train_list, bow_test, k=15)\n",
    "\n",
    "        # calculate confusion matrix and accuracy\n",
    "        confusion = np.zeros((15, 15), dtype=int)\n",
    "        for p, t in zip(pred, label_test_list):\n",
    "            confusion[t][p] += 1\n",
    "        accuracy = np.trace(confusion) / np.sum(confusion)\n",
    "    \n",
    "        # visualize_confusion_matrix(confusion, accuracy, label_classes, method_name=f\"bow_knn_dic_size={dic_size}\")\n",
    "        visualize_confusion_matrix(confusion, accuracy, label_classes, method_name=\"bow_knn\")\n",
    "    \n",
    "    return confusion, accuracy\n",
    "\n",
    "\n",
    "def predict_svm(feature_train, label_train, feature_test):\n",
    "    # To do\n",
    "    label_test_pred = []\n",
    "\n",
    "    for c in range(15):\n",
    "        # create a binary label for each class\n",
    "        binary_label_train = np.array([1 if lbl == c else 0 for lbl in label_train])\n",
    "        # C=0.1\n",
    "        # C=1\n",
    "        # C=10\n",
    "        C = 2\n",
    "        # LinearSVC classifier with the given C value\n",
    "        clf = LinearSVC(C=C, max_iter=10000)\n",
    "        clf.fit(feature_train, binary_label_train)\n",
    "        # predict the labels of the test features\n",
    "        confidence = clf.decision_function(feature_test)\n",
    "        label_test_pred.append(confidence)\n",
    "\n",
    "    # get the strongest prediction from the 15 classes\n",
    "    label_test_pred = np.argmax(np.array(label_test_pred), axis=0)\n",
    "    return label_test_pred\n",
    "\n",
    "\n",
    "def classify_svm_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list):\n",
    "    # To do\n",
    "    print(\"Extracting dense SIFT features\")\n",
    "    train_features = []\n",
    "    for img_path in img_train_list:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        feat = compute_dsift(img)\n",
    "        train_features.append(feat)\n",
    "\n",
    "    for dic_size in [600]:\n",
    "        vocab = build_visual_dictionary(train_features, dic_size)\n",
    "\n",
    "        print(\"Computing BoW histograms for training images\")\n",
    "        bow_train = []\n",
    "        for feat in train_features:\n",
    "            bow_train.append(compute_bow(feat, vocab))\n",
    "\n",
    "        print(\"Computing BoW histograms for test images\")\n",
    "        test_features = []\n",
    "        for img_path in img_test_list:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            feat = compute_dsift(img)\n",
    "            test_features.append(feat)\n",
    "\n",
    "        bow_test = []\n",
    "        for feat in test_features:\n",
    "            bow_test.append(compute_bow(feat, vocab))\n",
    "\n",
    "        print(\"Running SVM classifier\")\n",
    "        pred = predict_svm(bow_train, label_train_list, bow_test)\n",
    "\n",
    "        confusion = np.zeros((15, 15), dtype=int)\n",
    "        for p, t in zip(pred, label_test_list):\n",
    "            confusion[t][p] += 1\n",
    "        accuracy = np.trace(confusion) / np.sum(confusion)\n",
    "        \n",
    "        # visualize_confusion_matrix(confusion, accuracy, label_classes, method_name=f\"bow_svm_dic_size={dic_size}\")\n",
    "        visualize_confusion_matrix(confusion, accuracy, label_classes, method_name=\"bow_svm\")\n",
    "    \n",
    "    return confusion, accuracy\n",
    "\n",
    "def visualize_confusion_matrix(confusion, accuracy, label_classes,  method_name, out_dir=\"outputs\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    plt.title(\"accuracy = {:.3f}\".format(accuracy))\n",
    "    plt.imshow(confusion)\n",
    "    ax, fig = plt.gca(), plt.gcf()\n",
    "    plt.xticks(np.arange(len(label_classes)), label_classes)\n",
    "    plt.yticks(np.arange(len(label_classes)), label_classes)\n",
    "    # set horizontal alignment mode (left, right or center) and rotation mode(anchor or default)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"center\", rotation_mode=\"default\")\n",
    "    # avoid top and bottom part of heatmap been cut\n",
    "    ax.set_xticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    fig.tight_layout()\n",
    "    fname = os.path.join(out_dir, f\"{method_name}_confusion_acc.png\")\n",
    "    plt.savefig(fname, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    label_classes, label_train_list, img_train_list, label_test_list, img_test_list = extract_dataset_info(\"./scene_classification_data\")\n",
    "\n",
    "    classify_knn_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)\n",
    "    \n",
    "    classify_svm_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
